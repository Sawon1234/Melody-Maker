%% PNAStmpl.tex
%% Template file to use for PNAS articles prepared in LaTeX
%% Version: Apr 14, 2008


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BASIC CLASS FILE 
%% PNAStwo for two column articles is called by default.
%% Uncomment PNASone for single column articles. One column class
%% and style files are available upon request from pnas@nas.edu.
%% (uncomment means get rid of the '%' in front of the command)

%\documentclass{pnasone}
\documentclass{pnastwo}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Changing position of text on physical page:
%% Since not all printers position
%% the printed page in the same place on the physical page,
%% you can change the position yourself here, if you need to:

% \advance\voffset -.5in % Minus dimension will raise the printed page on the 
                         %  physical page; positive dimension will lower it.

%% You may set the dimension to the size that you need.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% OPTIONAL GRAPHICS STYLE FILE

%% Requires graphics style file (graphicx.sty), used for inserting
%% .eps files into LaTeX articles.
%% Note that inclusion of .eps files is for your reference only;
%% when submitting to PNAS please submit figures separately.

%% Type into the square brackets the name of the driver program 
%% that you are using. If you don't know, try dvips, which is the
%% most common PC driver, or textures for the Mac. These are the options:

% [dvips], [xdvi], [dvipdf], [dvipdfm], [dvipdfmx], [pdftex], [dvipsone],
% [dviwindo], [emtex], [dviwin], [pctexps], [pctexwin], [pctexhp], [pctex32],
% [truetex], [tcidvi], [vtex], [oztex], [textures], [xetex]

\usepackage{graphicx, hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% OPTIONAL POSTSCRIPT FONT FILES

%% PostScript font files: You may need to edit the PNASoneF.sty
%% or PNAStwoF.sty file to make the font names match those on your system. 
%% Alternatively, you can leave the font style file commands commented out
%% and typeset your article using the default Computer Modern 
%% fonts (recommended). If accepted, your article will be typeset
%% at PNAS using PostScript fonts.


% Choose PNASoneF for one column; PNAStwoF for two column:
%\usepackage{PNASoneF}
%\usepackage{PNAStwoF}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ADDITIONAL OPTIONAL STYLE FILES

%% The AMS math files are commonly used to gain access to useful features
%% like extended math fonts and math commands.

\usepackage{amssymb,amsfonts,amsmath}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% OPTIONAL MACRO FILES
%% Insert self-defined macros here.
%% \newcommand definitions are recommended; \def definitions are supported

%\newcommand{\mfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
%\def\s{\sigma}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Don't type in anything in the following section:
%%%%%%%%%%%%
%% For PNAS Only:
%\contributor{Submitted to Proceedings
%of the National Academy of Sciences of the United States of America}
%\url{www.pnas.org/cgi/doi/10.1073/pnas.0709640104}
%\copyrightyear{2008}
%\issuedate{Issue Date}
%\volume{Volume}
%\issuenumber{Issue Number}
%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% For titles, only capitalize the first letter
%% \title{Almost sharp fronts for the surface quasi-geostrophic equation}

\title{The Evolution of a Pianist}


%% Enter authors via the \author command.  
%% Use \affil to define affiliations.
%% (Leave no spaces between author name and \affil command)

%% Note that the \thanks{} command has been disabled in favor of
%% a generic, reserved space for PNAS publication footnotes.

%% \author{<author name>
%% \affil{<number>}{<Institution>}} One number for each institution.
%% The same number should be used for authors that
%% are affiliated with the same institution, after the first time
%% only the number is needed, ie, \affil{number}{text}, \affil{number}{}
%% Then, before last author ...
%% \and
%% \author{<author name>
%% \affil{<number>}{}}

%% For example, assuming Garcia and Sonnery are both affiliated with
%% Universidad de Murcia:
%% \author{Roberta Graff\affil{1}{University of Cambridge, Cambridge,
%% United Kingdom},
%% Javier de Ruiz Garcia\affil{2}{Universidad de Murcia, Bioquimica y Biologia
%% Molecular, Murcia, Spain}, \and Franklin Sonnery\affil{2}{}}

\author{
Debarghya Das\affil{1}{Cornell University},
Eric Chahin\affil{1}{Cornell University}
}

\contributor{CS 5724 - Final Project}

%% The \maketitle command is necessary to build the title page.
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{article}

\begin{abstract}
We present a method for evolving music created by piano in attempt to match \textit{any} song played on the piano. Typically, genetic algorithms are effective when searching large solution spaces where better solutions can be built from two pre-existing good solutions. We use these approaches, as well as some prevailing methods in signal processing to evolve sheet music that best match a piano song. We start with a random population set of sheet music, and evolve as it we try to maximize a fitness function (by minimizing the distance of our generated sound from the target). The originality of our work exists in, firstly, the use of certain timbral features of the cepstrum of the sound sample in defining an accurate and fast fitness function, and secondly, the use of no pre-existing data to determine fitness \footnote{As in Shazam \cite{wang}}. We talk about some different fitness functions we tried, and some successes and constraints of our system. 
\end{abstract}


%% When adding keywords, separate each term with a straight line: |
\keywords{piano detection | genetic algorithm | signal processing }

%% Optional for entering abbreviations, separate the abbreviation from
%% its definition with a comma, separate each pair with a semicolon:
%% for example:
%% \abbreviations{SAM, self-assembled monolayer; OTS,
%% octadecyltrichlorosilane}

% \abbreviations{}

%% The first letter of the article should be drop cap: \dropcap{}
%\dropcap{I}n this article we study the evolution of ''almost-sharp'' fronts

%% Enter the text of your article beginning here and ending before
%% \begin{acknowledgements}
%% Section head commands for your reference:
%% \section{}
%% \subsection{}
%% \subsubsection{}

\section{Introduction}
One of the problems we set out to tackle was "Given a piece played on a piano, can we match a combination of notes to that piece to best recreate it?". We can draw distinct similarities between this problem, and say, symbolic regression, which involves searching the space of mathematical expressions to find a model that best fits a certain dataset. \footnote{\url{http://jwork.org/scavis/wikidoc/doku.php?id=man:ai:symbolic_regression}}. 

Intuitively, the generation of music using Evolutionary Algorithms seems inherently applicable. Search algorithms that hunt through vast spaces of structures representing solutions to a problem while trying to optimize a certain fitness function, in attempt to simulate the natural process of evolution, seems well suited to a task such as generating music - which resides in a large intractable space. 

While other works \cite{lai} \cite{garcia} use Frequency Modulation synthesis to generate songs, our work uses a pre-defined set of piano notes \footnote{Obtained from University of Iowa Music Studios \url{http://theremin.music.uiowa.edu/MISpiano.html}} in order to attempt to reconstruct a sample of a given song. Several attempts have been made to use timbral features of audio signals to successfully classify them into genres \cite{germ} \cite{gtzan}. We therefore found it suitable to use timbral features in the fitness function of our EA as well. Unlike many popular music recognition software out there  \cite{wang}, however, we do not store a large pre-existing database of sounds with a fingerprint and simply compare timbral features to a database. Even with timbral features, it is not straightforward to accurately define distances between two audio samples in a way representative of their perceptive sound difference in reality. We begin with the initial features of the notes of a piano \footnote{Given that our audio signal data for the piano notes were obtained from a single source, several problems were encoutered due to the non-normalization of the notes across 2 of the 3 degrees of freedom of a piano - velocity of the stroke and duration of the aftertouch} and using precomputed timbral features of these notes, we attempt to derive fitness values of a certain arrangement of these notes with the timbral features of the sample sound. 

Eventually, we were able to evolve sounds that were near-identical to the sample sound over a long running time with one method, and evolve sounds somewhat identical to distantly resembling the sample sound in a fairly short amount of time with another method.

\section{Problem Definition}
We had initially set out to tackle two broad primary problems:
\begin{itemize}
\item Given a song, can we teach a computer to "learn" how to play it?
\item Can we teach a computer to algorithmically generate "good" music?
\end{itemize}
At close to the very outset, we dismissed the second problem. Teaching a computer to generate "good" music requires us to define a fitness function of what "good" represents. This has often been seen in literature \cite{biles} \cite{copley}. The inherent problem in defining a set of constraints to constitute what a fitness function inevitably ends up causing the sound to evolve to that explicit constraint, and not allowing for diversity. Using humans as a fitness function is an option, but this requires constraining the search space enough to avoid unavoidable bottle-necks in performing the evaluations. 

We therefore focussed on the first problem. As has been done in past work \cite{garcia}, we restricted our search space to a single instrument - the \textbf{piano}. This narrowed down our specific problem to the following:

\begin{quote} 
Given a digital representation of a sound sample played on the piano, can we successfully reproduce it by identifying the arrangement of notes or chords in which it was played?
\end{quote}


\section{Related Work}
The use of evolutionary algorithms at the sound level is concerned with the manipulation of parameters
that define a sound using a particular sound synthesis technique (SST), or with
parameters that define a particular deformation on an input stream (sound effects).
There are two broad categories of EA application in this area: as an optimization
technique for deriving the parameters of an accurate model of a particular sound
(usually a sampled sound) and for exploratory search in the investigation of new sounds. \cite{emcee} Although we initially set out to tackle both problems, we restrict ourselves to the former for the rest of the paper. 

When we optimize for a sample of sound, often from a traditional instrument (in our case, the piano), the sample sound is used as a target waveform. An EA is put to work to derive the parameters of a particular SST to produce a sound as close as possible to the target. A fitness function that measures the difference between a candidate sound and the target is usually employed and there are many technical issues involved in how best to define this. There are a
number of examples of successful uses of this approach, Garcia \cite{garcia} being one of the foremost. Garcia used Sound Synthesis Algorithms (SSAs) to evolve sounds and used an analytical distance metric that uses the complex spectrum of the target and test signals to test its accuracy. Similar methods have been adopted by Manzolli et al \cite{manzolli} and later by Lai et al \cite{lai}. It was modified for the use with stringed instruments by Riionheimo \cite{heimo}.

\begin{figure}
\centerline{\includegraphics[width=0.7\columnwidth]{img2/garciawork}}
\caption{An example of the results achieved by Garcia \cite{garcia} in his seminal work on synthesizing sound using evolutionary methods.}\label{fig1}
\end{figure}


In the past, several attempts have been made to make use of the timbral features characteristic of an audio signals to successfully classify them into genres \cite{germ} \cite{gtzan}. Given the success of the use of such features, we attempt to use the same features in our classification. 

\section{Method}
\subsection{Challenges}
Before we outline our approach to this problem, we outline several of the challenges we faced:
\begin{itemize}
\item \textbf{Availability of only 1 out of 3 degrees of freedom for the piano} The piano is an instrument with fewer degrees of freedom than most. The sound of a note on the piano is dependent on 3 factors - the key struck, the velocity at which the key is struck and the duration for which the key is held after being struck. Unfortunately, the online dataset we found for piano notes constrained us to exercising only the first degree of freedom. Our inability to normalize for the other degrees of freedom somewhat affecting the accuracy of our results in our second method. Our notes were of varying amplitudes and lengths.
\item \textbf{Arriving at an accurate fitness function for a sequence of notes and a sound sample} Accurately evaluating the fitness of audio samples is non-trivial. A lot of research in signal processing has been done to this effect, yet most methods remain slow. Most fast methods for "recognizing" audio signals involve large databases of hashes of spectral features in audio - databases that, given our application, were not possible to create beforehand. 
\item \textbf{Creating a fast fitness function for audio} With our fitness function, there was a notable tradeoff between speed and accuracy. Very accurate fitness functions involved recreating a waveform from the notes and performing a large comparison in the frequency domain, whereas a fast fitness function dealt with notes and the song itself in their frequency domains, and tried to compare spectral fingerprints.
\item \textbf{Experimentation without an FM synthesizer } Unlike many works in the past \cite{garcia}, we chose not to use a more robust, yet slow, FM synthesizer, but instead to work with existing audio samples of piano notes instead. This greatly diminishes the space we need to search in to synthesize our audio.
\item \textbf{Inherent performance constraints due to Python} We chose to use Python for our language of development because of it's speed of development. Unfortunately, this cost us in the performance front, and greatly reduced the realm of possibilities of fitness functions we could possibly test out because Python simply took too long to perform large computations on millions of numbers at a time, do FFTs, save large pickle files, etc.
\end{itemize}
\subsection{Representation}
Let the set of notes be $\mathbb{N}$. A piano has 7 octaves and a minor third, for a total of 88 keys. For our trials, we leave out the minor third, to give: 
$$\mathbb{N} = \{ C_i,  Db_i, D_i, Eb_i, E_i, F_i, Gb_i, G_i, Ab_i, A_i, Bb_i, B_i \}  \; \; \forall 1 \leq i \leq 7 $$

In our sample, $\|\mathbb{N}\| = 84 $. These above ordering in the notation is in ascending order from lowest to highest. Our representation $\mathbf{R}$ for a song of length $T$ milliseconds consists of a tuple of a list of set of notes, $(r_1, r_2, r_3, ... , r_n)$ and an integer representing the duration of it's beat in milliseconds, $P$. Formally, 

$$ \mathbf{R} = (P, (r_1, r_2, r_3, ... r_n ) ) \; P \in \mathbb{I}  $$
$$ r_i = \{ x | x \in \mathbb{N} \;x \;\text{is played at the } i^{th} \text{half-beat} \} \; \;  i < n$$
$$ \frac{nP}{8} = T $$

This representation reduces the space of possible notes, but fully represents everything "musically feasible" and representable by sheet music - a fair assumption. We had initially opted with a more general representation, a list of tuples of note and time of incidence sorted by the latter, but we discarded that representation for this one for its obvious advantages. We further elaborate on this representation in the further sections. 
\subsection{Methods}
Broadly, our high level approach, outlined in \ref{fig3} was a genetic algorithm, with crossover, mutation and selection. We varied the details of the crossover, mutation and selection function as well as maintained diversity with aging, and used the inherent linkage of our representation. Details of this are outlined in the next section. A large part of our task involved defining a computationally tractable, accurate fitness function. In that respect, we used two methods:
\begin{itemize}
\item \textbf{Continuous Sound Synthesis (CSS)} Very slow, very accurate.
\item \textbf{Spectral Peak Comparison (SPC)} Very fast, less accurate.
\end{itemize}
\subsubsection{Continuous Sound Synthesis (CSS)}
In this slower method, each time the fitness function is called with our representation, we use our python libraries to recreate the song from it's note structure from scratch, compute it's timbral features, and compare it with the ones of the sample we are striving to learn. This entire process, by virtue of being extremely computationally intensive, can take unto 30 seconds per evaluation, but can return extremely accurate evaluations. CSS models the interaction between the tail of one note and the start of another perfectly - much better than any model that individually looks at the spectral features of the notes and a sample. This makes it an extremely accurate fitness function. In layman terms, because I'm re-synthesizing an audio file from scratch for a member of my population, it's equivalent to replaying a song on the piano and listening to whether it sounds like the song the algorithm is striving to recreate. As you can imagine, overlaying notes even for small 10-20 second samples and computing it's mel-spectrogram is expensive.  

\subsubsection{Spectral Peak Comparison (SPC)}
In this faster method, we individually examine features of the individual notes of the piano and attempt to identify, from it's mel-spectrogram, defining audio fingerprints. Then, given a representation $R$ and a sample we want to synthesize $S$, we simply compare the spectral features at a given half-beat in the mel-spectrogram of $S$ to the notes in $R$ being played at that time. Let's first define some of the signal processing terms that we use:
 \begin{itemize}
    \item \textbf{A Spectrogram} - the audio signal in the frequency domain. After applying a Fast Fourier Transform (FFT) on the signal in discretized time bins, we can visualize the change in the distribution of frequency intensities throughout the song. \\
    \item \textbf{A Mel-Spectrogram} - Spectrograms are on a linear frequency scale of 0 to 22050 Hz (approximately the full audible range for humans). Humans hear certain frequencies more easily than others, so it makes sense to attenuate the spectrogram accordingly. The mel scale is a non-linear frequency scale that preserves frequencies that humans are more sensitive to. A mel-spectrogram is created by applying a triangular filterbank to a spectrogram. \\
    \item \textbf{MFCCs (Mel-frequency cepstral coefficients)} - MFCCs are manually engineered audio features that have been shown to work well in practice. They are the "spectrogram of the spectrogram", or the cepstrum, and are typically computed from the log-amplitudes of the mel frequencies, running a filterbank of size 26 with a Discrete Cosine Transform on the log-powers, and using the first 13 numbers (from the $\textless$ 15000 Hz frequency range). Sometimes we ignore the first value of a DCT because it's only indicative of the cumulative amplitude of a signal at that point. With the deltas and delta-deltas, we get 36 such features for each window. \\
\begin{figure}
\centerline{\includegraphics[width=0.9\columnwidth]{img2/jbmfcc}}
\caption{The visualization of the 36 MFCCs of \textbf{Jingle Bells}}\label{fig4}
\end{figure}
 \item \textbf{Spectral peaks} - MFCCs are fairly accurate representations of audio data, but the size of MFCCs are still fairly large (36 x total number of windows). Instead, a well-known technique in signal processing is to use the peaks of mel-spectrogram as an identifier, or "fingerprint", for a audio signal. Visually, if you picture the mel-spectrogram to be a 3D surface, the location (and heights) of those peaks are distinctive to a given audio signal. 
\begin{figure}
\centerline{\includegraphics[width=0.9\columnwidth]{img2/jbpeaks}}
\caption{The visualization of the mel-spectrogram of \textbf{Jingle Bells}. The black dots denote spectral peaks at critical frequencies. This version of Jingle Bells was played with the representation $(1500, (\{E5\}, \{\},  \{E5\}, \{\}, \{E5\}, \{\}, \{\}, \{\}, \{E5\}, \{\}, \{E5\}, \{\}, \{E5\},  \\ \{\}, \{\}, \{\}, \{E5\}, \{\}, \{G5\}, \{\}, \{C5\}, \{\}, \{\}, \{D5\}, \{E5\}, ...) )$}\label{fig5}
\end{figure}
\end{itemize}



\subsection{Example}
Let's use Jingle Bells to demonstrate our representation. The sheet music for Jingle Bells can be represented as follows. The period is on the first row, and each subsequent row has 4 parts - a note, a period number it belongs to (1 indexed) and a beat number (1 indexed), and the lyric at that beat/halfbeat. Recall that there are 4 beats in one period. 
\begin{verbatim}
Period: 1500 milliseconds
E5 1 1 (Jin-)
E5 1 2 (-gle)
E5 1 3 (bells)
E5 2 1 (Jin-)
E5 2 2 (-gle)
E5 2 3 (bells)
E5 3 1 (Jin-)
G5 3 2 (-gle)
C5 3 3 (all)
D5 3 4.5 (the)
E5 4 1 (way)
\end{verbatim}

This would be translated to the following in our representation:
\begin{align*}
R_{jb} =  (1500,( & \{E5\}, \{\},  \{E5\}, \{\}, \{E5\}, \{\}, \{\}, \{\},  \\
& \{E5\}, \{\}, \{E5\}, \{\}, \{E5\}, \{\}, \{\}, \{\},\\
&  \{E5\}, \{\}, \{G5\}, \{\}, \{C5\}, \{\}, \{\}, \{D5\}, \\
& \{E5\})  )
\end{align*}

\begin{figure}
\centerline{\includegraphics[width=0.9\columnwidth]{img2/classical}}
\caption{The visualization of the spectrogram (above) and the mel-scaled spectrogram (below) of the first 30 seconds of \textbf{Wolfgang Amadeus Mozart}'s \textbf{Requiem (K 626)}.}\label{fig3}
\end{figure}


\section{System Architecture and Implementation}
NOTE TO ERIC - explanation of log file? add other GA stuff. fix mis-uses of beats/periods in the paper. Add (more) result graphs. Review stuff i wrote. Make sure to run makeIndex + bibtex to generate references. FIX images. Sections left - System architecture + implementation, evaluation (methodology, results), discussions, conclusion. Add googdoc stuff.
\begin{figure*}
\centerline{\includegraphics[width=0.7\paperwidth]{img2/evocomp}}
\caption{A high level outline of our genetic algorithm.}\label{fig2}
\end{figure*}

\section{Evaluation}
\subsection{Methodology}
\subsection{Results}
\begin{figure}
\centerline{\includegraphics[width=0.9\columnwidth]{img2/fullscale}}
\caption{The performance of fitness of a GA (in green) vs the Hill Climber (in blue) for SPC for \textbf{full scale}. Note, that while -4.0 fitness is achieved, this number is inaccurate to a certain degree, as explained in the section on SPC.}\label{fig7}
\end{figure}

\begin{figure}
\centerline{\includegraphics[width=0.9\columnwidth]{img2/clocks}}
\caption{The performance of fitness of a GA (in green) vs the Hill Climber (in blue) for SPC for \textbf{clocks} \footnote{The introduction of the song Clocks, by Coldplay}. Note, that while -16.0 fitness is achieved, this number is inaccurate to a certain degree, as explained in the section on SPC.}\label{fig8}
\end{figure}

\begin{figure}
\centerline{\includegraphics[width=0.9\columnwidth]{img2/result}}
\caption{The visualization of the evolutionary learnt full scale mel spectrogram (a series of notes of increasing pitch) of one octave (above) and the original mel spectrogram (below). We see a clear resemblance and the noticeable increasing trend of pitch with time.}\label{fig4}
\end{figure}


\subsection{Discussion}
0. overall, good results
1. trouble in doing chords
2. tradeoff between speed and accuracy
\section{Future Work}
\begin{itemize}
\item Use a faster programming language like C in order to be able run more computationally intensive tasks quicker.
\item Test out more fitness functions, like spectral norm, and try to optimize the speed - accuracy tradeoff for fitness functions.
\item Explore more accurate results using piano data using all 3 degrees of freedom.
\item Figure out a better way of representing chords.
\item Use an FM synthesis technique to widen the range of signals we can synthesize. 
\item Use more intricate representations of our genotype. For example, we could use autocorrelation to determine repeating sub-parts of a song, and similar to symbolic regression, we could attempt to find functional units of notes that are often repeated.  
\item Evolve metrics to strive toward to with a well-defined Sound Synthesis Technique (SST) to be able to achieve the larger goal of synthesizing unknown music algorithmically. 
\end{itemize}
\section{Conclusion}

\section{Acknowlegements}
We would like to thank Professor Lipson for all his feedback and inspiration that helped us with this project, and express great sorrow at his decision to move to Columbia University following this semester. We would like to wish him the best on his journey.

\begin{figure*}[ht]
\centerline{\includegraphics[width=0.9\paperwidth]{img2/scale}}
\caption{The individual mel-spectrogram of each of the notes in the middle octave of the piano. \footnote{All of these notes have been reduced to the same length, although they are in actuality of different lengths}}\label{fig6}
\end{figure*}
%% == end of paper:

%% Optional Materials and Methods Section
%% The Materials and Methods section header will be added automatically.

%% Enter any subheads and the Materials and Methods text below.
%\begin{materials}
% Materials text
%\end{materials}

%% Optional Appendix or Appendices
%% \appendix Appendix text...
%% or, for appendix with title, use square brackets:
%% \appendix[Appendix Title]

%\begin{acknowledgments}
%-- text of acknowledgments here, including grant info --
%
%Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
%\end{acknowledgments}

%% PNAS does not support submission of supporting .tex files such as BibTeX.
%% Instead all references must be included in the article .tex document. 
%% If you currently use BibTeX, your bibliography is formed because the 
%% command \verb+\bibliography{}+ brings the <filename>.bbl file into your
%% .tex document. To conform to PNAS requirements, copy the reference listings
%% from your .bbl file and add them to the article .tex file, using the
%% bibliography environment described above.  

%%  Contact pnas@nas.edu if you need assistance with your
%%  bibliography.

% Sample bibliography item in PNAS format:
%% \bibitem{in-text reference} comma-separated author names up to 5,
%% for more than 5 authors use first author last name et al. (year published)
%% article title  {\it Journal Name} volume #: start page-end page.
%% ie,
% \bibitem{Neuhaus} Neuhaus J-M, Sitcher L, Meins F, Jr, Boller T (1991) 
% A short C-terminal se\quence is necessary and sufficient for the
% targeting of chitinases to the plant vacuole. 
% {\it Proc Natl Acad Sci USA} 88:10362-10366.

%% Enter the largest bibliography number in the facing curly brackets
%% following \begin{thebibliography}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Adding Figure and Table References
%% Be sure to add figures and tables after \end{article}
%% and before \end{document}

%% For figures, put the caption below the illustration.
%%
%% \begin{figure}
%% \caption{Almost Sharp Front}\label{afoto}
%% \end{figure}


%% For Tables, put caption above table
%%
%% Table caption should start with a capital letter, continue with lower case
%% and not have a period at the end
%% Using @{\vrule height ?? depth ?? width0pt} in the tabular preamble will
%% keep that much space between every line in the table.

%% \begin{table}
%% \caption{Repeat length of longer allele by age of onset class}
%% \begin{tabular}{@{\vrule height 10.5pt depth4pt  width0pt}lrcccc}
%% table text
%% \end{tabular}
%% \end{table}

%% For two column figures and tables, use the following:

%% \begin{figure*}
%% \caption{Almost Sharp Front}\label{afoto}
%% \end{figure*}

%% \begin{table*}
%% \caption{Repeat length of longer allele by age of onset class}
%% \begin{tabular}{ccc}
%% table text
%% \end{tabular}
%% \end{table*}
\end{article}
\end{document}

